<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>网络编程</title>
    <link href="/2024/06/21/networkProgramming/"/>
    <url>/2024/06/21/networkProgramming/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"/><p>学习网络编程前，我们需要了解到网络编程种使用到的基础协议。</p><h1 id="TCP-UDP协议"><a href="#TCP-UDP协议" class="headerlink" title="TCP&#x2F;UDP协议"></a>TCP&#x2F;UDP协议</h1><h2 id="OSI-网络七层模型"><a href="#OSI-网络七层模型" class="headerlink" title="OSI 网络七层模型"></a>OSI 网络七层模型</h2><p>为使不同计算机厂家的计算机能够互相通信，以便在更大的范围内建立计算机网络，有必要建立一个国际范围的网络体系结构的标准。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635594756086-57f94165-da81-4cd0-bc9a-d6c5e040d2e9.png#averageHue=%23ebe7e2&clientId=u3a4e5b82-6605-4&from=paste&height=515&id=ub1167634&originHeight=515&originWidth=1296&originalType=binary&ratio=1&rotation=0&showTitle=false&size=242306&status=done&style=none&taskId=u6e8fa34f-77c0-4cc5-804d-281e543734d&title=&width=1296" alt="image.png"><br>从底至顶：<br>物理层：使原始的数据比特流能在物理介质上传输。<br>数据链路层：通过校验、确认和反馈重发等手段，形成稳定的数据链路。<br>网络层：进行路由选择和流量控制（IP协议）<br>传输层：提供可靠的端口到端口的数据传输服务（TCP&#x2F;UDP协议）<br>会话层：负责建立、管理和终止进程之间的会话和数据交换<br>表示层：负责数据格式转换、数据加密与解密、压缩与解压缩等<br>应用层：为用户的应用进程提供网络服务</p><p>应表会传网数物<br>我们主要了解一下传输层及应用层的原理，如果有兴趣进一步了解的话，可以找一些计算机网络原理的书籍的查阅一下。</p><h3 id="传输控制协议TCP"><a href="#传输控制协议TCP" class="headerlink" title="传输控制协议TCP"></a>传输控制协议TCP</h3><p>传输控制协议TCP是Internet一个重要的传输层协议。TCP提供面向连接、可靠、有序的字节流传输服务。应用程序在使用TCP之前，必须先建立TCP连接。<br>数据报文的样式：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635837937595-9507926b-a462-4abc-96a8-317ca219adf9.png#averageHue=%239b9c9b&clientId=u04d42337-f640-4&from=paste&height=447&id=u13c11ada&originHeight=447&originWidth=1283&originalType=binary&ratio=1&rotation=0&showTitle=false&size=179789&status=done&style=none&taskId=u368e58e9-d826-4ba5-b970-d3f7e2deef9&title=&width=1283" alt="image.png"></p><h4 id="TCP握手机制"><a href="#TCP握手机制" class="headerlink" title="TCP握手机制"></a>TCP握手机制</h4><p>TCP三次握手建立连接的意义不在于打通网络，而是在传输之前检测网络的互通性，目的就是最大程度上面检测网络的互通性。<br>具体步骤：</p><ol><li>客户端发送确认请求给服务端，让服务端准备确认</li><li>服务端收到请求后，会返回针对这次的确认请求的结果响应，回复客户端(ack_sql &#x3D; x+1)，当服务端回复了确认的信息，客户端就认为相互之间能够正常建立请求了。</li><li>客户端开始发送建立连接的请求，但不等于就直接建立连接，要端服两方都确认建立连接才会真正建立。</li><li>服务端响应建立连接的请求</li></ol><p>这个过程就好比打电话的时候双方先喂一下（对应步骤1，2），然后A说：我要开始讲了（步骤3），B回复说：你讲吧 我听着（步骤4）<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635838627100-409e6ef3-8cb6-4e07-a707-0e28679ab2ce.png#averageHue=%23fafafa&clientId=u04d42337-f640-4&from=paste&height=808&id=udf5dad18&originHeight=808&originWidth=732&originalType=binary&ratio=1&rotation=0&showTitle=false&size=176789&status=done&style=none&taskId=u3f2d8e84-7147-4142-8990-95793520009&title=&width=732" alt="image.png"><br>四次挥手：<br>断开连接也是需要双方确认，不能随便断开。<br>步骤：</p><ol><li>客户端发送断开的请求到服务端</li><li>服务端将客户端对应的请求标识为半关闭状态</li><li>客户端收到了响应之后，开始走自己的逻辑关闭，等待释放，因为有可能有在途中数据没有处理完 或者在等待服务端发送未完成的数据</li><li>等待一段时间之后，服务端会主动发送一条等待确认的消息给客户端，告诉客户端，服务端已经可以关闭连接了</li><li>客户端响应这个连接等于说认同服务端关闭的操作，等服务端接收到确认就会关闭连接</li><li>双方都关闭连接</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635841050728-4caaa8cc-1019-4a8f-a388-673e2f2effb1.png#averageHue=%23f8f8f8&clientId=u04d42337-f640-4&from=paste&height=771&id=uac261ad8&originHeight=771&originWidth=732&originalType=binary&ratio=1&rotation=0&showTitle=false&size=176136&status=done&style=none&taskId=ud558d3a9-6ba5-494a-8da2-4ee6c7dbcc6&title=&width=732" alt="image.png"></p><h3 id="用户数据报协议UDP"><a href="#用户数据报协议UDP" class="headerlink" title="用户数据报协议UDP"></a>用户数据报协议UDP</h3><p>UDP是Internet传输层协议。提供无连接、不可靠、数据尽力传输服务，不保证数据一定能传达到。它的数据包结构也比较简单。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635846365266-90df9f40-42e4-446e-a3fa-3f5c1567da36.png#averageHue=%23eaeaea&clientId=u04d42337-f640-4&from=paste&height=612&id=u80380861&originHeight=612&originWidth=897&originalType=binary&ratio=1&rotation=0&showTitle=false&size=112608&status=done&style=none&taskId=u9d148b82-a355-4300-9d6d-0ee1631bf1d&title=&width=897" alt="image.png"><br>作为开发人员，我们在使用UDP来构建应用，关注几点：</p><ol><li>应用进程更容易控制发送什么数据以及何时发送</li><li>无需建立连接</li><li>无连接状态</li><li>首部开销小</li></ol><h3 id="UDP和TCP比较"><a href="#UDP和TCP比较" class="headerlink" title="UDP和TCP比较"></a>UDP和TCP比较</h3><table><thead><tr><th>TCP</th><th>UDP</th></tr></thead><tbody><tr><td>面向连接</td><td>无连接</td></tr><tr><td>提供可靠保证</td><td>不可靠</td></tr><tr><td>慢</td><td>快</td></tr><tr><td>资源占用多</td><td>资源占用少</td></tr></tbody></table><p>快慢是TCP与UDP相对的。物联网、视频语音通讯这些领域使用UDP就很合理。</p><h2 id="Socket编程"><a href="#Socket编程" class="headerlink" title="Socket编程"></a>Socket编程</h2><p>是一种Internet中应用最广泛得网络应用编程接口，操作系统帮我们实现与3种底层协议接口：</p><ul><li>数据报类型套接字SOCK_DGRAM（面向UDP接口）</li><li>流式套接字SOCK_STREAM（面向TCP接口）</li><li>原始套接字SOCK_RAM（面向网络层协议接口IP、ICMP等）</li></ul><p>主要socket API 机器调用过程：<br>创建套接字-&gt;端点绑定-&gt;发送数据-&gt;接收数据-&gt;释放套接字<br>Socket API：<br>listen()、 accept() 只用于服务端；<br>connect()只用于客户端；<br>socket() 、bind()、 send()、recv()、sendto()、recvfrom()、close()<br>socket编程了解一下就行了，属于比较底层的东西，在他的基础上JAVA有很良好的网络编程解决方案，目前用得最多的应该式Netty</p><h1 id="网络编程"><a href="#网络编程" class="headerlink" title="网络编程"></a>网络编程</h1><h2 id="BIO"><a href="#BIO" class="headerlink" title="BIO"></a>BIO</h2><p>最开始的时候Java如何去提供一个网络编程的实现？这部分我们直接上代码~：<br>在服务端我们需要创建一个ServerSocket，并且绑定指定的端口:<br><code>ServerSocket serverSocket = new ServerSocket(8080)；</code><br>然后通过accept阻塞地拿去过来的请求：<br><code>Socket request = serverSocket.accept();</code><br>通过Socket对象可以获取客户端的请求，需要加上IO操作，所以JAVA程序中需要net + io 才能实现一个BIO的程序：<br><code>InputStream input = request.getInputStream();</code></p><p>而在客户端这边需要创建Socket对象，而非ServerSocket对象：<br><code>Socket socket = new Scoket(&quot;localhost&quot;,8080);</code><br>利用IO中的outputStream来发送消息给服务端：<br><code>OutputStream out = socket.getOutputStream();</code><br>使用OutputStream的write方法把消息发送到服务端，write()也是阻塞的：<br><code>String msg = &quot;moumoumou&quot;；</code><br><code>out.write(msg.getBytes(&quot;UTF-8&quot;));</code></p><p>由于服务端是阻塞的，所以我们只能一个一个地处理请求，等于说一对一的服务，对于服务端来说这样的处理方式肯定是不合理的，需要对此进行优化，想到阻塞的实际是阻塞线程，是不是如果我们将收到请求后处理的部分代码写在一个线程中就能满足我们处理多个请求的需求呢？<br>于是乎有了这样的一个版本</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">BIOServer1</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-type">ExecutorService</span> <span class="hljs-variable">threadPool</span> <span class="hljs-operator">=</span> Executors.newCachedThreadPool();<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-type">ServerSocket</span> <span class="hljs-variable">serverSocket</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ServerSocket</span>(<span class="hljs-number">8080</span>);<br>        System.out.println(<span class="hljs-string">&quot;tomcat 服务器启动成功&quot;</span>);<br>        <span class="hljs-keyword">while</span> (!serverSocket.isClosed()) &#123;<br>            <span class="hljs-type">Socket</span> <span class="hljs-variable">request</span> <span class="hljs-operator">=</span> serverSocket.accept();<br>            System.out.println(<span class="hljs-string">&quot;收到新连接 : &quot;</span> + request.toString());<br>            threadPool.execute(() -&gt; &#123;<br>                <span class="hljs-keyword">try</span> &#123;<br>                    <span class="hljs-comment">// 接收数据、打印</span><br>                    <span class="hljs-type">InputStream</span> <span class="hljs-variable">inputStream</span> <span class="hljs-operator">=</span> request.getInputStream();<br>                    <span class="hljs-type">BufferedReader</span> <span class="hljs-variable">reader</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedReader</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">InputStreamReader</span>(inputStream, <span class="hljs-string">&quot;utf-8&quot;</span>));<br>                    String msg;<br>                    <span class="hljs-keyword">while</span> ((msg = reader.readLine()) != <span class="hljs-literal">null</span>) &#123; <span class="hljs-comment">// 阻塞</span><br>                        <span class="hljs-keyword">if</span> (msg.length() == <span class="hljs-number">0</span>) &#123;<br>                            <span class="hljs-keyword">break</span>;<br>                        &#125;<br>                        System.out.println(msg);<br>                    &#125;<br>                    System.out.println(<span class="hljs-string">&quot;收到数据,来自：&quot;</span>+ request.toString());<br>                &#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>                    e.printStackTrace();<br>                &#125; <span class="hljs-keyword">finally</span> &#123;<br>                    <span class="hljs-keyword">try</span> &#123;<br>                        request.close();<br>                    &#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>                        e.printStackTrace();<br>                    &#125;<br>                &#125;<br>            &#125;);<br>        &#125;<br>        serverSocket.close();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>当我们accept接受到新的请求的时候，就起了一个新的线程来单独处理这个请求，而不阻塞主线程。为了节省资源，用了线程池。<br>这个服务端已经可以同时处理多个客户端请求了。<br>服务端可能需要支持浏览器的访问，浏览器使用的是http协议，明显我们的程序并没有对协议的支持。这里就体现了协议与程序之前的关系。<br>我们要做到能支持协议，就需要明白协议是如何解析数据包的，这里用http协议举例：<br>这个是http请求的结构<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635950370495-3c83a906-ec43-41a0-baf8-a534653ef323.png#clientId=ue4b50aa6-39d9-4&from=paste&height=644&id=u21eadb04&originHeight=644&originWidth=1769&originalType=binary&ratio=1&rotation=0&showTitle=false&size=640660&status=done&style=none&taskId=uf41f9d83-9704-4e90-bf2e-9758ba7bdea&title=&width=1769" alt="image.png"><br>http响应的结构：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635950529316-176241df-b7e2-4165-b1f2-aeeb9d8ea251.png#clientId=ue4b50aa6-39d9-4&from=paste&height=661&id=u5e665659&originHeight=661&originWidth=1788&originalType=binary&ratio=1&rotation=0&showTitle=false&size=363480&status=done&style=none&taskId=ucbb222cc-1d07-4dc3-989d-e1c1c35275d&title=&width=1788" alt="image.png"><br>由于这部分也不是重点只是举个例子，就不多赘述了。言归正传，要使我们刚刚的程序支持http，只需要在响应上加上给协议需要的数据结构就行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">OutputStream</span> <span class="hljs-variable">outputStream</span> <span class="hljs-operator">=</span> request.getOutputStream();<br>outputStream.write(<span class="hljs-string">&quot;HTTP/1.1 200 OK\r\n&quot;</span>.getBytes());<br>outputStream.write(<span class="hljs-string">&quot;Content-Length: 11\r\n\r\n&quot;</span>.getBytes());<br>outputStream.write(<span class="hljs-string">&quot;Hello World&quot;</span>.getBytes());<br>outputStream.flush();<br></code></pre></td></tr></table></figure><p>对于这样的服务端程序，其实局限还是很大的，为了能开发出更理想的服务端，我们不得不搞清楚目前的问题出在哪里。<br>先来理解一下之前说的阻塞的定义，以及线程同步异步的定义，以便更好地进行优化：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635951608132-20d6323e-2a46-42aa-943f-23eda29278af.png#clientId=ue4b50aa6-39d9-4&from=paste&height=549&id=u5ee319de&originHeight=549&originWidth=1786&originalType=binary&ratio=1&rotation=0&showTitle=false&size=414739&status=done&style=none&taskId=u973ea9f4-3db2-46ad-adb2-d4c9059b6e8&title=&width=1786" alt="image.png"><br>同步与阻塞是两个不同的理论，可以阻塞获取资源 ，然后异步处理，也可以非阻塞地获取资源，然后同步处理（应该不会有这种应用场景吧…）<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1635951702321-bdb19d55-d6b0-49d3-8bb2-81d71b6e98f5.png#clientId=ue4b50aa6-39d9-4&from=paste&height=317&id=u2fa71f3a&originHeight=317&originWidth=1791&originalType=binary&ratio=1&rotation=0&showTitle=false&size=190083&status=done&style=none&taskId=ud9d5f127-283d-445a-9014-a17d618d54c&title=&width=1791" alt="image.png"><br>为了解决阻塞这个最大问题，NIO应运而生。</p><h2 id="NIO"><a href="#NIO" class="headerlink" title="NIO"></a>NIO</h2><p>早在Java 1.4，就已经提供了新的JAVA IO操作非阻塞API，想去替代JAVA IO 和JAVA Networking相关的API。<br>NIO中有三个核心组件ByteBuffer （缓冲区）、 Channel（通道） 、Selector（选择器）</p><h3 id="ByteBuffer"><a href="#ByteBuffer" class="headerlink" title="ByteBuffer"></a>ByteBuffer</h3><p>Buffer本质上是一个可以写入数据的内存块（类似数组），也可以再次读取，内存卡包含在NIO Buffer对象中，该对象提供了一组方法，可以更轻松地使用内存块。明显是针对数组这种不方便的数据结构而产生的一种数据结构。Buffer API相对数组更容易操作和管理。<br>使用Buffer进行读写，需要如下四个步骤：</p><ol><li>将数据写入缓冲区</li><li>调用buffer.flip()，转换位读取模式</li><li>进行数据读取</li><li>调用buffer.clear() 或者 buffer.compact() （清除已读数据）清除缓冲区内数据。</li></ol><h4 id="ByteBuffer-工作原理"><a href="#ByteBuffer-工作原理" class="headerlink" title="ByteBuffer 工作原理"></a>ByteBuffer 工作原理</h4><p>在Buffer中有三个重要的属性：</p><ul><li>capacity 容量：作为一个内存块，Buffer具有一定的大小，且是固定的，即是容量</li><li>postition 位置：写入模式下就是写入数据的起始位置，读取模式下就是读取数据的起始位置</li><li>limit 限制：写入模式下，等于buffer的容量，读取模式下，等于已写入的数据量</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1637135130966-8874ce3e-7a65-4111-95e8-77ad84527d1c.png#clientId=u049b0928-2628-4&from=paste&height=543&id=uc1054ec5&originHeight=543&originWidth=1421&originalType=binary&ratio=1&rotation=0&showTitle=false&size=178053&status=done&style=none&taskId=u985dcf42-b1cc-40fd-8ad3-8ede8762f3b&title=&width=1421" alt="image.png"><br>这个API的使用就直接写代码去理解了，没必要用过多的文字帮忙理解。</p><h4 id="ByteBuffer内存类型"><a href="#ByteBuffer内存类型" class="headerlink" title="ByteBuffer内存类型"></a>ByteBuffer内存类型</h4><p>ByteBuffer默认在堆中为我们申请内存（非直接内存），它还额外为性能关键型的代码提供了直接内存（Direct Memory）的实现。<br>堆外内存通过ByteBuffer.allocateDirect(bytes) 就可以实现申请。<br>堆外内存的好处在于：</p><ul><li>进行网络IO 或者文件IO时 比堆内内存少一次拷贝。正常来说是file&#x2F;socket &lt;-&gt; os memory &lt;-&gt; jvm heap ，由于GC会移动内存地址的原因，在写file或socket的过程中，为防止GC把数据地址改变导致找不到数据的问题，数据会先被移到堆外后，再写入，这个是操作系统帮我们实现的。</li><li>堆外内存在GC的范围之外，降低了GC的压力，并且实现了自动管理：DirectByteBuffer 中有一个Cleaner对象（PhantopmReference），Cleaner被GC前会执行clean()方法，出发DirectByteBuffer中定义的DeAllocator。</li></ul><p>使用堆外内存的建议：</p><ul><li>能显著提高性能的时候菜去使用；</li><li>分配给一些大型、寿命长的数据</li><li>通过虚拟机参数MaxDirectMemorySize限制大小，防止耗光物理的的内存。</li></ul><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>buffer装载好数据通过channel传递出去<br>对比之前的BIO ：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1637138676342-bde35f40-614c-4e49-95dd-86de530da802.png#clientId=u049b0928-2628-4&from=paste&height=495&id=u9e01f023&originHeight=495&originWidth=1672&originalType=binary&ratio=1&rotation=0&showTitle=false&size=391275&status=done&style=none&taskId=u9c7ca5a8-f506-43b7-8e16-d0959d53144&title=&width=1672" alt="image.png"><br>在之前的BIO编程中，通过socket + IOStream来完成，由对应的IO包与NET包协同完成，而NIO中 我们通过Buffer + Channel 来实现，且这两个实现都是由NIO包提供的。 Channel的API涵盖了UDP&#x2F;TCP网络和文件IO ，包括FileChannel、DatagramChannel、SocketChannel、ServerSocketChannel ，<br>Channel 肩负了创建网络连接 以及 处理数据 的工作。和标准的IO Stream操作相比，在同一个Channel中对数据进行读取和写入，而不需要Input Output Stream两个对象，Stream通常都是单向的；可以非阻塞地将数据读取和写入Channel；Channel 始终读取或写入缓冲区。</p><h4 id="ServerChannel"><a href="#ServerChannel" class="headerlink" title="ServerChannel"></a>ServerChannel</h4><p>用于建立TCP网络连接，类似java.net.Socket。<br>有两种创建SocketChannel形式：</p><ul><li>客户端主动发起与服务器的连接</li><li>服务端获取的新连接<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//客户端主动发起连接方式</span><br><span class="hljs-type">SocketChannel</span> <span class="hljs-variable">socketChannel</span> <span class="hljs-operator">=</span> SocketChannel.open();<br><span class="hljs-comment">//设置为非阻塞模式</span><br>socketChannel.configureBlocking(<span class="hljs-literal">false</span>);<br>socketChannel.connect(<span class="hljs-keyword">new</span> <span class="hljs-title class_">InetSocketAddress</span>(<span class="hljs-string">&quot;127.0.0.1&quot;</span>, <span class="hljs-number">8080</span>));<br><span class="hljs-comment">//向通道写入数据</span><br>channel.write(byteBuffer);<br><span class="hljs-comment">//通道从缓冲区中读取数据，读取响应</span><br><span class="hljs-type">int</span> <span class="hljs-variable">bytesRead</span> <span class="hljs-operator">=</span> socketChannle.read(byteBuffer);<br><br><span class="hljs-comment">//关闭连接</span><br>scoketChannel.close();<br></code></pre></td></tr></table></figure>可以看到我们可以直接操作Channel来进行读写，需要注意的是write()和read()方法都是非阻塞的，writer()可能尚未写入任何内容就返回了，所以一般会在循环中调用writer()，而read()方法则可能没读取到数据直接返回，所以需要根据返回的int值来判断读取的字节数。<br>ServerSocketChannel可以监听新建的TCP连接通道，用来替换BIO的ServerSocket。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 创建网络服务端</span><br><span class="hljs-type">ServerSocketChannel</span> <span class="hljs-variable">serverSocketChannel</span> <span class="hljs-operator">=</span> ServerSocketChannel.open();<br>serverSocketChannel.configureBlocking(<span class="hljs-literal">false</span>); <span class="hljs-comment">// 设置为非阻塞模式</span><br>serverSocketChannel.socket().bind(<span class="hljs-keyword">new</span> <span class="hljs-title class_">InetSocketAddress</span>(<span class="hljs-number">8080</span>)); <span class="hljs-comment">// 绑定端口</span><br><span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>    <span class="hljs-type">SocketChannel</span> <span class="hljs-variable">socketChannel</span> <span class="hljs-operator">=</span> serverSocketChannel.accept(); <span class="hljs-comment">// 获取新tcp连接通道</span><br>    <span class="hljs-keyword">if</span>(sockerChannel != <span class="hljs-literal">null</span>)&#123;<br>        <span class="hljs-comment">//处理请求</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>主要的是ServerSocketChannel.accept()如果配置成非阻塞的，如果没有挂起的连接，该方法会立即返回null。同时 必须检查返回的SocketChannel是否为null。23：25</li></ul>]]></content>
    
    
    <categories>
      
      <category>network</category>
      
    </categories>
    
    
    <tags>
      
      <tag>network</tag>
      
      <tag>IO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>zookeeper</title>
    <link href="/2024/06/18/zookeeper/"/>
    <url>/2024/06/18/zookeeper/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"/><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>ZooKeeper（后文简称ZK）是一种用于分布式应用程序的性能协调服务，提供一种集中式的信息存储服务<br>特点：数据存储在内存中，类似文件系统的树型结构（文件和目录），高吞吐和低延迟，集群高可靠<br>作用：基于ZooKeeper可以实现分布式统一配置中心，服务注册中心，分布式锁等功能<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645672636332-65e246bd-4b9a-4f2e-a01b-1155b03d9fb6.png#averageHue=%23f9f5f2&clientId=u9a40121b-87d1-4&from=paste&height=466&id=u71b5976b&originHeight=466&originWidth=1785&originalType=binary&ratio=1&rotation=0&showTitle=false&size=240509&status=done&style=none&taskId=u9f6fc748-f8af-4a54-bcca-7b6134ec2d2&title=&width=1785" alt="image.png"></p><p>单机系统的处理能力有限，且可用性和可靠性都比较低，所以需要分布式系统，特别在大型的互联网公司对数据可靠的要求非常高。于是将原本单体系统会被拆分成非常多的细微的小的系统，可能在单体上的一个服务会被分配到多个服务，多个计算节点协同一起完成。这个过程会出现什么问题？这种拆分服务去协同工作是有顺序要求的，就需要有一种机制来协调节点来按照我们想要的顺序要完成分配的计算任务。还有一点，在单系统中会出现资源竞争，会碰到多线程竞争产生的线程安全问题，所以在原来的单个系统会采用锁的机制来保证程序的正确性。而这种情况也会出现在分布式系统里面发生，且这个线程可能就已经不是单个进程中的多线程问题了，而是不同节点中多个进程的多线程问题了，于是引入了分布式锁来处理这类问题。<br>服务调用顺序的协调，资源竞争的协调都是分布式协调服务的作用。当把协调服务中公共基础部分抽取出来做成一个独立的公共的基础服务供大家使用，这就是分布式协调服务。免去了在多个分布式系统里面重复的工作。ZooKeeper就是这样的一种分布式协调服务的提供者。在基于分布式协调服务之上，我们可以实现具体的功能：注册中心、分布式锁。<br>ZooKeeper是雅虎在分布式实践过程中产生的产品<br>应用案例：<br>Hbase 使用ZK进行Master选举，服务间协调<br>Solr使用ZK进行集群管理、Leader选举、配置管理<br>dubbo使用ZK来完成服务注册和发现<br>Mycat 使用ZK来集群管理、配置管理<br>Sharding-sphere 使用ZK来集群管理、配置管理<br>ZK同类产品：<br>consul 国外较多，用途和ZK类似<br>etcd 轻量级的<br>Doozer 高可用 完整一致性，小量且非常重要的数据场景</p><p>ZK官网：<a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p><p>ZK搭建可以参考官网的指导，也可以通参考文档《Zookeeper安装手册》</p><p>CLI-操作命令<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645675190606-a8635103-c095-4663-ad87-a75d3d81e007.png#averageHue=%23e7e7e7&clientId=u9a40121b-87d1-4&from=paste&height=793&id=ubb4af44e&originHeight=793&originWidth=1459&originalType=binary&ratio=1&rotation=0&showTitle=false&size=176686&status=done&style=none&taskId=ufca8e719-2c3e-4649-8b9d-3cf5a6f4571&title=&width=1459" alt="image.png">setquota，设置配额，表示节点下可以有多少个子节点，但是并不是强制的，原本指定3，但是你还是创建第4个，但是会记录一条异常日志，表示超长。</p><p>API操作<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645688379168-79925492-d1b6-4c0b-bbf6-7f51d12184ca.png#averageHue=%23e8e8e8&clientId=u9a40121b-87d1-4&from=paste&height=844&id=u5bb5595d&originHeight=844&originWidth=1425&originalType=binary&ratio=1&rotation=0&showTitle=false&size=213256&status=done&style=none&taskId=ubdc0c9c9-2209-4d13-be72-0edd4654951&title=&width=1425" alt="image.png"><br>第三方客户端<br>zkClient<br>Curator</p><h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><p>zk的核心概念分三块 session、数据模型、watch</p><h2 id="Session会话"><a href="#Session会话" class="headerlink" title="Session会话"></a>Session会话</h2><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645689277144-64fe0c58-0e1f-4a03-bec9-1a4641a7a388.png#averageHue=%23f9f4ee&clientId=ubf509c67-8447-4&from=paste&height=442&id=u97ccd3c0&originHeight=442&originWidth=1239&originalType=binary&ratio=1&rotation=0&showTitle=false&size=186131&status=done&style=none&taskId=u041d4b15-d4e4-4fef-bd57-b40992efabd&title=&width=1239" alt="image.png"><br>客户端要连接到服务端，成功连接就会产生一个会话。会话在zk中属于比较重要的概念。</p><ol><li>一个客户端连接一个会话，由zk分配唯一会话ID</li><li>客户端以特定的时间间隔(tickTime)发送心跳以保持会话有效</li><li>超过会话超时时间未收到客户端的心跳，则判定客户端死了，这个会话超时时间默认为两倍的tickTIme，通过maxSessionTimeout 与minSessionTimeout可以配置这个超时值</li><li>会话中请求按FIFO的顺序执行</li></ol><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p>zk提供了集中式的数据存储，存储形式是类似与unix的文件系统，名称空间树的形式</p><ul><li>类似unix文件类型 ，以&#x2F; 为根</li><li>区别unix：节点可以包含与之关联的数据以及子节点（既是文件也是文件夹）</li><li>节点的路径总是标识为规范的、绝对的、斜杠分隔的路径</li></ul><p>一般来讲我们把zk上的数据节点称为znode<br>对于znode有如下性质：</p><ul><li>名称：名称唯一，命名规范</li><li>类型：节点有几种类型：持久、顺序、临时、临时顺序</li><li>数据：节点有它的数据构成</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645689956385-5d2490de-5797-4428-9999-82120e9af040.png#averageHue=%23f7f7f7&clientId=ubf509c67-8447-4&from=paste&height=427&id=u3bfd1dfa&originHeight=427&originWidth=877&originalType=binary&ratio=1&rotation=0&showTitle=false&size=81208&status=done&style=none&taskId=u7e203c1b-d1dd-4263-b0c1-d800c5211fe&title=&width=877" alt="image.png"></p><h3 id="znode"><a href="#znode" class="headerlink" title="znode"></a>znode</h3><h4 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h4><p>节点名称有如下限制，除了下述限制外能使用其余任意的unicode字符</p><ul><li>null（\u0000）不能作为路径名的一部分</li><li>\u0001-\u0019 和\u007F-\u009F 不能使用，因为它们不能很好的显示，会以奇怪的方式显示</li><li>\ud800-uf8fff（感觉这里有点问题），\uFFF0-\uFFFF</li><li>“.”字符可以用作另一个名称的一部分，但是”.”和”..”不能当都用于指示路径上的节点，因为zk不能使用相对路径<br>“&#x2F;a&#x2F;b&#x2F;.&#x2F;c” 或“c&#x2F;a&#x2F;b&#x2F;..&#x2F;”为无效内容。</li><li>“zookeeper”为保留节点名</li></ul><h4 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h4><ul><li>持久节点 <code>create /app1 666</code> app1为节点名称，666为数据。没有名称或者数据都是创建不了节点的。</li><li>临时节点 <code>create -e /app2 888</code></li><li>顺序节点 <code>create -s /app1/cp 888</code> </li><li>临时顺序节点 <code>create -e -s /app1/ 888</code></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645700625932-346b814e-3980-4278-879b-5980282debdd.png#averageHue=%230c0906&clientId=ua5136b71-28ae-4&from=paste&height=49&id=u50d51ef1&originHeight=49&originWidth=532&originalType=binary&ratio=1&rotation=0&showTitle=false&size=4876&status=done&style=none&taskId=u4d72d2a8-c80b-4179-908f-74d2424fdbf&title=&width=532" alt="image.png"><br>可以看到真正创建出来的节点是以app1为前缀后面补了10位序列号。<br>        <code>create -s /app1/ aa</code>   0000000001<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645700741711-1352f651-d398-43d7-a2db-5276b5fdf8b2.png#averageHue=%230b0805&clientId=ua5136b71-28ae-4&from=paste&height=45&id=u1cbb03cc&originHeight=45&originWidth=570&originalType=binary&ratio=1&rotation=0&showTitle=false&size=4665&status=done&style=none&taskId=ud71dd0da-6142-4b3f-96ba-aed15b2eb2b&title=&width=570" alt="image.png"><br>这个没有指定节点名，就会直接以序列为名的节点。</p><ol><li>序号为10位十进制序号</li><li>每个父节点一个计数器 </li><li>计数器是带符号int（4字节）到2147483647之后将溢出（导致名称变为“<path-2147483647>”）</li></ol><p>临时节点会在会话结束时被删除。</p><h4 id="数据构成"><a href="#数据构成" class="headerlink" title="数据构成"></a>数据构成</h4><p><strong>节点数据</strong>：存储的协调数据（状态信息、配置、位置信息等）</p><p><strong>数据量上限</strong>：1M  存储数据只是为了完成协调服务，不需要存储很大的量。<br><strong>节点元数据</strong>（stat结构） ： 当我们执行get之后会得到大串的返回<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645708427465-3b0e5472-1b7b-4f27-9dde-66f64de72f4f.png#averageHue=%23070503&clientId=ua5136b71-28ae-4&from=paste&height=284&id=u5114b182&originHeight=284&originWidth=585&originalType=binary&ratio=1&rotation=0&showTitle=false&size=19695&status=done&style=none&taskId=u5573a9a7-28f4-40d2-8f7b-f37c55151a5&title=&width=585" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645708448420-3023364f-0ade-4f20-8f3b-2c65b4911c05.png#averageHue=%23dcdbdb&clientId=ua5136b71-28ae-4&from=paste&height=804&id=u8bda75ff&originHeight=804&originWidth=1188&originalType=binary&ratio=1&rotation=0&showTitle=false&size=218304&status=done&style=none&taskId=u8108bdd8-7594-473e-b7ae-4ad96837401&title=&width=1188" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645751357710-48419291-79a7-4f92-b627-e9902cc4a47b.png#averageHue=%23100a09&clientId=u8cf64b7c-fe99-4&from=paste&height=741&id=uf63f573a&originHeight=741&originWidth=1124&originalType=binary&ratio=1&rotation=0&showTitle=false&size=122381&status=done&style=none&taskId=u5fbab828-0b7d-48cc-a4b4-3796ef4d574&title=&width=1124" alt="image.png"><br>另外，值得一提的是create命令在创建节点时能指定path 和data， 还可以指定 <a href="https://zookeeper.apache.org/doc/r3.7.0/zookeeperProgrammers.html#sc_ZooKeeperAccessControl">ACL</a> (access control )访问控制，形式为scheme:expression,perms  -&gt; ip:19.22.0.0&#x2F;16,read 这就表示ip在19.22.0.0-16有读的权限，具体在使用时可以参考官网。<br>这个功能虽然用的少但是要知晓。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645751960297-9586d6e9-e952-4366-88d1-678229063d19.png#averageHue=%23f6f3ef&clientId=u8cf64b7c-fe99-4&from=paste&height=220&id=ud0fa667d&originHeight=220&originWidth=806&originalType=binary&ratio=1&rotation=0&showTitle=false&size=31987&status=done&style=none&taskId=u00e7f6a8-65eb-46d0-9f63-3fcf44169ce&title=&width=806" alt="image.png"></p><p>对于分布式协调服务，顺序是一个十分重要的概念，所以能够控制顺序的时间就举足轻重。<br>zookeeper中有多种方式来跟踪时间<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645752516232-cc9682e2-5b3c-498d-aa9f-08a6a0bbce27.png#averageHue=%23cdcdcd&clientId=u8cf64b7c-fe99-4&from=paste&height=708&id=u74ae7c8c&originHeight=708&originWidth=1706&originalType=binary&ratio=1&rotation=0&showTitle=false&size=353695&status=done&style=none&taskId=uf6516a62-ccb9-4761-81c7-6b231955372&title=&width=1706" alt="image.png"></p><h2 id="watch监听机制"><a href="#watch监听机制" class="headerlink" title="watch监听机制"></a>watch监听机制</h2><p>非常重要。<br>客户端可以在znodes上设置watch，监听znode的变化。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645752893516-5a35d7d4-6701-4436-9ce3-02576f3b1e78.png#averageHue=%23f6f6f6&clientId=u8cf64b7c-fe99-4&from=paste&height=698&id=ud0e4dbc0&originHeight=698&originWidth=804&originalType=binary&ratio=1&rotation=0&showTitle=false&size=102732&status=done&style=none&taskId=u58effa35-feb2-417d-8b74-86ee7ac83f6&title=&width=804" alt="image.png"><br>通过监听节点，我们可以知道节点是否存在，是否被删除，节点数据变化，节点下子节点的变化等消息。watch是个boolean类型入参。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645753285888-7e15de8b-fe56-421d-8206-a4b24f7d2976.png#averageHue=%23060402&clientId=u8cf64b7c-fe99-4&from=paste&height=457&id=u699191b7&originHeight=457&originWidth=533&originalType=binary&ratio=1&rotation=0&showTitle=false&size=30388&status=done&style=none&taskId=u3028a343-37a7-4157-a23c-852ac692d70&title=&width=533" alt="image.png"><br>比如当我们执行命令<code>get /study 1</code>时会在这个节点上部建一个watch<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645753702742-a6db25b0-c8fc-4312-9ff5-fce1b8171a9f.png#averageHue=%23070503&clientId=u8cf64b7c-fe99-4&from=paste&height=272&id=u52ca187a&originHeight=272&originWidth=493&originalType=binary&ratio=1&rotation=0&showTitle=false&size=18444&status=done&style=none&taskId=u736bd684-eff2-4db6-88ef-8750c3cdc64&title=&width=493" alt="image.png"><br>我们再开第二个客户端连接上zk，去修改这个节点时候：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645753790169-2511f096-fd98-4efe-913a-94c133d2e023.png#averageHue=%230f0e0c&clientId=u8cf64b7c-fe99-4&from=paste&height=282&id=u5c90fa4d&originHeight=282&originWidth=588&originalType=binary&ratio=1&rotation=0&showTitle=false&size=20407&status=done&style=none&taskId=u9f898573-d1e0-4326-8ed4-381427d13d0&title=&width=588" alt="image.png"><br>原来的客户端会收到消息<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645753816889-35cb96db-c67f-436d-b081-94133aaebda8.png#averageHue=%23070504&clientId=u8cf64b7c-fe99-4&from=paste&height=353&id=ucd1e3f55&originHeight=353&originWidth=626&originalType=binary&ratio=1&rotation=0&showTitle=false&size=24726&status=done&style=none&taskId=u5cf6be9d-566f-40fb-9c9a-6772c7d5340&title=&width=626" alt="image.png"><br>但是如果你再进行一次修改之后，并不会再触发这个消息。</p><h3 id="种类"><a href="#种类" class="headerlink" title="种类"></a>种类</h3><p>watch有两种:</p><ul><li>data watch 监听数据变化</li><li>child watch 监听子节点变化</li></ul><h3 id="事件"><a href="#事件" class="headerlink" title="事件"></a>事件</h3><p>watch事件有四种:</p><ul><li>Created event : Enabled with a call to exites</li><li>Deleted event ：Enabled with a call to exists , getData , getChildren</li><li>Changed event: Enabled with a call to exists and getData</li><li>Child event : Enabled with a call to getChildren</li></ul><p>可以看到getData()，getChildren()， exists() 三种方法都会触发事件。</p><h3 id="重要特性"><a href="#重要特性" class="headerlink" title="重要特性"></a>重要特性</h3><p>一次性触发：watch触发之后就会被删除，要持续监控变化，就需要持续设置watch<br>有序性：客户端先得到watch通知，之后才会看到变化结果。<br>更详细地说:</p><ol><li>顺序一致性（Sequential Consistency） ：保证客户端操作是按顺序生效的</li><li>原子性（Atomicity）：更新成功或失败，没有部分成功或失败的情况</li><li>单个系统镜像：无论连接到哪个服务端，客服端看到的结果都是相同的内容</li><li>可靠性：数据的变更不会丢失，除非是被客户端覆盖。每个变更在zk中都是有日志记录的，当有请求进服务端来操作数据时会先写日志，再进行实际的数据操作。如果是集群部署的zk还要发起对应的集群流程。</li><li>及时性：保证系统的客户端当时读取到的数据是最新的。集群内也会及时同步。</li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>在使用watch的时候要注意以下几点：</p><ul><li>watch是一次性触发器，如果你获得了一个watch时间，并且希望得到关于未来变更的通知，则必须设置另一个watch</li><li>因为watch是一次性触发器，并且在获取时间和发送watch的新请求之间存在延迟，所以不可能可靠地得到节点发生的每个改变，如果对变化有非常强的要求，需要注意这一点。</li><li>一个watch对象只会被特定的通知触发一次。如果一个watch对象同时注册了exits、getData，当节点被删除时，删除事件对exits、getData都有效，但是会调用一次watch通知。</li></ul><p>后续自行写个小demo体验zookeeper的api使用。</p><h1 id="典型应用场景"><a href="#典型应用场景" class="headerlink" title="典型应用场景"></a>典型应用场景</h1><p>zk用途广泛，能用于</p><ul><li>数据发布订阅（配置中心）：解决配置问题</li><li>命名服务：标识集群里面的某个服务，当它还没有准备好的时候，我们可以不受干扰的开发其他需要调用这个服务的代码或者部署</li><li>Master选举</li><li>集群管理：实时地动态对集群中节点的加入与退出</li><li>分布式队列</li><li>分布式锁</li></ul><h2 id="配置中心"><a href="#配置中心" class="headerlink" title="配置中心"></a>配置中心</h2><p>正常来说，不比单体应用，分布式系统中配置众多，各个系统之间的关系繁多，每个系统里面或多或少都有些配置信息，并且很可能在都有部分相同的配置项，分布式应用服务众多，于是如何解决系统参数配置及动态改参就成了一个问题。<br>配置中心帮我们解决这个问题，在服务启动时需要用到某个配置时，就去配置中心里面去取。当配置中心的配置发生改动，所有用到该配置的服务都能及时感知到。这个就是配置中心为我们做的事情，统一管理，并且动态配置。<br>如何用zk实现配置中心？经过上面zk的学习我们知道zk的节点可以存储数据，并且节点发生变动就watch机制可以通知到关心数据变化的节点，利用这两点就能实现。<br>我们可以一个<strong>配置项</strong>一个znode，也可以一个<strong>配置文件</strong>一个znode。就比如我们可以把mycat的scheme.xml文件直接存放在zk上当作一个节点，在mycat启动时去zk上获取节点就能获得这个文件。就算再多的mycat实例启动起来，我们也只需要维护这一份scheme.xml就可以了。如果是不同的服务但是有相同的配置项，那就可以采取第一种，将某个配置项作为节点存储到zk。</p><p>参见ConfigCenterDemo.java</p><h2 id="命名服务"><a href="#命名服务" class="headerlink" title="命名服务"></a>命名服务</h2><p>在系统开发的过程间，不同系统由不同开发小组完成，A服务已经开发完成，但是B服务并没有完成开发。但是A服务的开发者会去负责别的项目的开发，如果等到B服务开发完成再来联调修改什么的操作，对A来说会比较麻烦。能不能达到一个效果就是只要B服务一旦部署上线，A服务中用到B服务的功能就能正常运转，不再需要开发者回过头来再关注A的部署。这时候用到的手段就叫命名服务。B服务只需要给自己的服务命一个名称，然后A服务知道了这个服务名，就可以到命名服务里面去取到它的服务提供详情。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645815722259-3f0ec48c-138c-4f91-ac18-7efbbdb10c17.png#averageHue=%23fefefe&clientId=u25a8184d-a926-4&from=paste&height=385&id=u291f3e06&originHeight=770&originWidth=1231&originalType=binary&ratio=1&rotation=0&showTitle=false&size=225009&status=done&style=none&taskId=ue13ee0c8-7fa0-483a-a299-3eebb7a1b90&title=&width=615.5" alt="image.png"><br>如何解决服务A可以动态得到服务B的调用地址呢？<br>首先A服务去zk上注册一个service B的节点的watch，当B服务准备好的时候将自己注册上zk，就是创建对应的service B节点。</p><h2 id="Master选举"><a href="#Master选举" class="headerlink" title="Master选举"></a>Master选举</h2><p>在分布式系统中，主从集群是常用的结构，一主多从，主节点负责协调管理集群，主节点一般只有一个，当主节点故障之后，集群还需要保证可用，那么就必须重新选举主节点。这种选举的实现的方式由很多，zk只是其中一种。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645815925808-5a4cb59f-8d50-4a54-89fb-cc0778aee1f1.png#averageHue=%23fdfdfc&clientId=u25a8184d-a926-4&from=paste&height=357&id=ubbd1f534&originHeight=713&originWidth=1299&originalType=binary&ratio=1&rotation=0&showTitle=false&size=237226&status=done&style=none&taskId=u9fca9864-84ca-4224-853f-d4d0b6d6c30&title=&width=649.5" alt="image.png"><br>zk如何帮助我们实现选举？<br>所有从节点都去zk上创建个名称相同的节点，谁成功了，谁就成为了新的主节点，假如实例一创建节点成功，成为了主节点，其他的实例创建就会收到节点已存在的返回，这时候其他节点就只要获取主节点信息就可以了。 主节点必须是个临时节点，因为当实例1宕机的时候，节点就会自动被删除，其他节点才有机会创建新的master节点。<br>除上述过程之外，我们还可以把集群中所有可用的节点都注册称为servers节点，通过这个节点我们就能了解到集群的情况。当实例1称为了主节点之后，它可能需要获取到集群的所有信息，来做管理，这时候servers节点就起到了作用。所有的节点都会在servers下面注册个临时&#x2F;临时顺序节点，这样就方便主节点进行集群管理。具体根据业务需要来实现。如果是顺序节点的存放方式，我们还可以用最小节点的方式来实现选举，即谁的序号最小谁就去当主节点，这样就不需要争抢。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645815998311-09606917-278d-4b26-a561-eccdf4fb4d6d.png#averageHue=%23f6f6f5&clientId=u25a8184d-a926-4&from=paste&height=345&id=u3b6c7659&originHeight=689&originWidth=1404&originalType=binary&ratio=1&rotation=0&showTitle=false&size=297390&status=done&style=none&taskId=ubfd6ca42-5186-4491-b5a4-2c8cf2b0cb8&title=&width=702" alt="image.png"><br>注意，节点宕机就会会话中断，中断时间默认为2倍TikTime，所有消息的通知也会有这么长时间的延迟。</p><p>代码实现见MasterElectionDemo.java</p><h2 id="分布式队列"><a href="#分布式队列" class="headerlink" title="分布式队列"></a>分布式队列</h2><p>原理是利用了顺序节点的性质。入队的过程就是创建顺序节点，顺序就是入队顺序，生产者将数据创建为顺序节点放入zk即可。出队，消费者取所有的子节点，移除最小号节点。<br>有个重要的问题，如果是一个无界的队列，那么没有什么问题，一直入队就可以了。如果是要实现一个有界队列，那么就需要生产者在放的时候判断节点下子节点的个数。这时候入队就没有那么随意了，需要用到分布式锁来判断是否超了队列的大小，每个生产者都要抢到锁之后才能取放入。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645817073769-83853268-78e0-4619-bcb5-126bc99c0670.png#averageHue=%23fefefe&clientId=u25a8184d-a926-4&from=paste&height=366&id=u73b23b55&originHeight=731&originWidth=1424&originalType=binary&ratio=1&rotation=0&showTitle=false&size=228536&status=done&style=none&taskId=u34fa3301-bd5d-4451-9b61-b77ded580ed&title=&width=712" alt="image.png"><br>这个留给你去试下，入队的put操作，出队的get操作。</p><h2 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h2><p>zk中实现锁的方式有两种：</p><ol><li>创建同名的临时节点</li></ol><p>失败的实例创建watch监听，获得锁的人执行完代码后就要把节点删除，需要抢锁的人就会被通知到去抢。为什么不创建持久节点呢？就是怕抢到锁的实例出现故障宕机，于是就成了死锁问题。<br>这种实现的缺点就是由于只有一个会抢锁成功，所以失败的实例阻塞之后等到再抢锁的时候，会触发成百上千的通知，把所有需要抢锁的实例都“惊醒”。并发量太大就不要采用这种方式。<br>再者如果不是先可重入的锁，那么就会出现如果线程获得锁之后进入下一段代码有需要获取这个锁，就一定拿不到这个锁，等于拿着锁去找锁，死锁了。使用可重入锁同时也要注意解锁条件，必须全部释放才算是完全解锁。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645817747769-e7e47989-9e0d-4d9f-af38-df7e2d9f8610.png#averageHue=%23f69085&clientId=u25a8184d-a926-4&from=paste&height=492&id=u09a5fda1&originHeight=984&originWidth=1998&originalType=binary&ratio=1&rotation=0&showTitle=false&size=473392&status=done&style=none&taskId=u65edb995-95e9-40aa-94ce-7b695378a79&title=&width=999" alt="image.png"><br>代码见ZkDistributeLock.java</p><ol start="2"><li>创建临时顺序节点</li></ol><p>类似银行办公区号等叫号。<br>取号了之后，等唤醒，交由节点序号比自己小的节点来唤醒。2号等1号通知，3号等2号通知…watch只需要注册在需要比自己小的那一个上面就可以了。 锁由最小号的序号获得。这样挨个唤醒的方式解决了第一种的惊群效应。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645818448424-a167d807-a0a3-4429-a9e1-49b05a1cabf3.png#averageHue=%23fef9f6&clientId=u25a8184d-a926-4&from=paste&height=507&id=u8e305e50&originHeight=1014&originWidth=1955&originalType=binary&ratio=1&rotation=0&showTitle=false&size=575940&status=done&style=none&taskId=ucbcac81b-e779-48f1-89a9-c781dd07a5d&title=&width=977.5" alt="image.png"><br>代码见ZkDistributeImproveLock.java</p><h1 id="ZooKeeper集群"><a href="#ZooKeeper集群" class="headerlink" title="ZooKeeper集群"></a>ZooKeeper集群</h1><p>仅仅使用一个zk，可靠性肯定是不高的，zk集群能够提供更高的zk服务。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645844257100-a77f300b-0eaf-4e06-8502-7caa58309764.png#averageHue=%23faf5f0&clientId=u41e4239b-724c-4&from=paste&height=477&id=u5853250b&originHeight=477&originWidth=1226&originalType=binary&ratio=1&rotation=0&showTitle=false&size=194245&status=done&style=none&taskId=u532081f4-244a-4fda-a48c-c039968f0e7&title=&width=1226" alt="image.png"></p><ul><li>可靠的ZooKeeper服务</li><li>只要集群中的大多数（过半的机器）都准备好，服务就可用</li><li>容错集群设置至少需要三个服务器，强烈建议使用奇数个服务器</li><li>建议每个服务运行在单独的机器上</li><li>其余节点都需要连接到Leader节点接受管理</li></ul><h2 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h2><p>集群的搭建本来是很简单的。<br>只需要增加server.1及之后的配置来告诉集群节点信息。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645972850541-604678ab-2417-47bc-a4c7-2b05b35c3ca9.png#averageHue=%23757370&clientId=u0f708a63-ac69-4&from=paste&height=615&id=u7ae5b7d7&originHeight=615&originWidth=624&originalType=binary&ratio=1&rotation=0&showTitle=false&size=175104&status=done&style=none&taskId=ubfb27000-9a65-4412-9296-8ed192ad8ce&title=&width=624" alt="image.png"><br>配置说明</p><ul><li>initLimit<br>集群中的follower服务器（F）与Leader 服务器（L） 之间完成初始化同步连接时能容忍的最多心跳数（TickTime的数量）。如果zk集群环境数量确实很大，同步数据的事件会变长，因此这种情况下可以适当调大该参数。简单来说就是新来的机器需要初始化，连接上去初始化的超时限制的5倍的TickTime。</li><li>syncLimit<br>集群中Follower服务器与Leader服务器之间请求和应答之间能容忍的最多心跳数（TickTIme的数量），同步限制的时间。</li><li>集群节点<br>server.id &#x3D; host:port:port<br>id：通过在各自的dataDir目录下创建一个名为myid的文件来为每台机器服务一个服务器id，这个要自己去创建。</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1645973579489-e56645bd-1c6a-4b91-9ee0-882a9f3006c3.png#averageHue=%23eeeeee&clientId=u0f708a63-ac69-4&from=paste&height=317&id=rQWda&originHeight=317&originWidth=1712&originalType=binary&ratio=1&rotation=0&showTitle=false&size=70234&status=done&style=none&taskId=u5444b6e5-5769-4150-b3ab-c7126ff7fe9&title=&width=1712" alt="image.png"><br>    两个端口号：第一个Follower连接到Leader用的端口，第二个在选举Leader时用的</p><h2 id="连接集群"><a href="#连接集群" class="headerlink" title="连接集群"></a>连接集群</h2><p>集群的所有节点都可以提供服务，客户端连接时，连接串可以指定多个或全部集群节点的连接地址。如”10.168.1.23:2181,10.168.1.24:2181,10.168.1.25:2181”当一个节点不通的时候，客户端将自动切换到另一个节点。<br>zk对服务器的要求还是建议给予一个单独的服务器，硬盘没有什么需求，主要是内存有个2G+就可以了。</p><p>此处省略搭建过程，如有需要可以参考文档。</p><h2 id="集群监控"><a href="#集群监控" class="headerlink" title="集群监控"></a>集群监控</h2><p><a href="http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_monitoring">官方说明</a><br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1646011844309-90af9673-de78-49e1-b1fc-8aa8dd55ab4f.png#averageHue=%23fcfaf9&clientId=u4ba4ea45-1c7d-4&from=paste&height=136&id=ue8af93ca&originHeight=136&originWidth=638&originalType=binary&ratio=1&rotation=0&showTitle=false&size=9306&status=done&style=none&taskId=ud13835c4-2ecf-4ad8-9224-3900b3512f3&title=&width=638" alt="image.png"></p><ul><li><a href="https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_zkCommands">四字监控命令</a><br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1646011949420-32194ba6-2d67-4808-b21b-a1b0da4f08cc.png#averageHue=%23f9f7f5&clientId=u4ba4ea45-1c7d-4&from=paste&height=930&id=ude960703&originHeight=930&originWidth=1748&originalType=binary&ratio=1&rotation=0&showTitle=false&size=194212&status=done&style=none&taskId=u03eba3ad-7843-454e-84d4-66144f8bcb7&title=&width=1748" alt="image.png"></li><li><a href="https://zookeeper.apache.org/doc/current/zookeeperJMX.html">JMX</a><br>zk支持使用JMX来进行监控。<blockquote><p>The Java JDK ships with a simple JMX console named <a href="http://java.sun.com/developer/technicalArticles/J2SE/jconsole.html">jconsole</a> which can be used to connect to ZooKeeper and inspect a running server. Once you’ve started ZooKeeper using QuorumPeerMain start <em>jconsole</em>, which typically resides in <em>JDK_HOME&#x2F;bin&#x2F;jconsole</em><br>When the “new connection” window is displayed either connect to local process (if jconsole started on the same host as Server) or use the remote process connection.</p></blockquote></li></ul><h2 id="集群-ZAB协议"><a href="#集群-ZAB协议" class="headerlink" title="集群-ZAB协议"></a>集群-ZAB协议</h2><p>ZAB协议（Zookeeper Atomic Broadcast ） ZK原子消息广播协议是专门为zk设计的一种数据一致性协议。<br>集群能够保证服务的可用性，容错性。集群中每一台服务器存储的都是完整的数据。那么集群中的节点如何保证这种数据一致性？并且为什么我们需要leader节点？<br>当客户端连接上集群之后，集群无外乎做两种操作，一个是读取，一个是更新。对于读取，在任何一个节点都是可以提供的。但是对于更新的请求，就不是随便哪个节点就可以完成的。更新的工作应该只能由leader节点统一负责，非leader节点接收到了更新操作都需要转交给leader节点统一操作。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1646012574045-c6230ae8-09be-4e18-a8c0-69f0aaffc066.png#averageHue=%23f8f8f8&clientId=u4ba4ea45-1c7d-4&from=paste&height=617&id=u50c78594&originHeight=617&originWidth=1385&originalType=binary&ratio=1&rotation=0&showTitle=false&size=157062&status=done&style=none&taskId=u2746ceae-e37a-4bfb-8cd5-2a902cda662&title=&width=1385" alt="image.png"><br>如图 read的请求可以直接访问db然后就返回出去了。但是写请求进来了就需要由leader接受并发起原子广播（Atomic Broadcast），超过半数的从节点同意更新，才会更新。</p><h3 id="同步过程说明"><a href="#同步过程说明" class="headerlink" title="同步过程说明"></a>同步过程说明</h3><ol><li>所有事务请求会转发给Leader节点</li><li>Leader节点会创建一个提议（Propose），这个提议会带上一个全局单调递增的事务id（zxid），然后广播出去。</li><li>Follower处理提议，并且作出反馈，是否同意这个提议</li><li>Leader节点收到过半反馈，广播commit。</li><li>Leader来response</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1646038677419-efa92c8d-710f-4ef4-bd70-20b181c2f62a.png#averageHue=%23f5f5f5&clientId=u510575cd-2d84-4&from=paste&height=677&id=u8846068a&originHeight=677&originWidth=1780&originalType=binary&ratio=1&rotation=0&showTitle=false&size=238675&status=done&style=none&taskId=ub0fd3a3b-8270-4cc2-b9d4-d9e2500f837&title=&width=1780" alt="image.png"><br>ZAB保证有序性这个重要的性质。</p><h3 id="崩溃恢复"><a href="#崩溃恢复" class="headerlink" title="崩溃恢复"></a>崩溃恢复</h3><p>除了保证有序性，ZAB协议还能为我们实现崩溃恢复。<br>Leader节点如果出现宕机，或者说由于网络问题原因导致Leader服务器失去了与过半Follower的联系，那么就会进入崩溃恢复模式。</p><ul><li>ZAB协议规定如果一个事务（Propose）在一台机器上被处理成功，那么应该在所有机器上都要被处理成功，哪怕是机器出现故障</li><li>ZAB协议确保了在Leader服务器上已提交的事务最终会再所有服务器上提交</li><li>ZAB协议确保丢弃那些只在Leader服务器上被提出的事务，意味着如果在提交之前leader宕机时候的提议都会被抛弃。</li></ul><p>所以ZAB协议设计的选举算法应该满足两点:</p><ol><li>确保提交 已经被leader提交的事务&#x2F;提议</li><li>确保丢弃 已被跳过的事务&#x2F;提议</li></ol><p>如果让Leader选举算法能够保证选举出来的新的Leader服务器拥有集群中所有机器最高的ZXID的事务Proposal，那么就能保证这个新选举出来的Leader一定具有所有已经提交的提案。如果让具有最高编号Proposal的机器来称为Leader，甚至可以省去Leader服务器检查Proposal的提交与丢弃。</p><h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><p>Leader选举出来之后，Follower需要与Leader进行数据同步，当集群中半数节点完成同步，那么集群就可以正常提供服务。同步过程：</p><ul><li>Leader节点会为每个F提供一个队列，并将没有被F节点同步的事务以Proposal的形式逐个发送到F节点，并在每一个Proposal消息后面在发送一个commit消息，用以表示该事务已被提交。具体来讲就是如果zxid最到到10，如果F连接上来是8，L提供的队列里面就是9的Proposal，然后是9的commit，再是10的Proposal，然后再是10的commit，F就会从这个列表中取出数据同步到自己的服务上。</li><li>F服务器将所有未同步的事务都从L节点上同步过来并成功应用到本地数据库中，L服务器会将F加入到真正可用的节点列表中，才开始其他流程。</li></ul><h3 id="丢弃事务提案处理"><a href="#丢弃事务提案处理" class="headerlink" title="丢弃事务提案处理"></a>丢弃事务提案处理</h3><p>通过前面的学习我们知道，全局单调递增的事务ID - ZXID 非常重要，上面的实现全是建立在这个基础之上的。所以我们来了解一下这个zxid。<br>在ZAB协议中的事务编号ZXID设计中，ZXID是要给64位的数字：</p><ul><li>低32位 是一个简单的单调递增计数器。针对客户端的每一个事务请求，L服务器在产生一个新的事务提案的时候都会对该计数器进行+1操作</li><li>高32位 代表了Leader周期纪元的编号。每当选举产生一个新的L服务器，就会从这个L服务器上取出其本地日志中最大事务提案的ZXID，并从中解析出纪元值，再对其进行+1操作，以新值作为新的纪元，再将低32位置零开始生成新的ZXID。</li></ul><p>基于这样的策略，当一个包含了上一个Leader周期中尚未提交过的事务提案的服务器加入到集群中时，此时集群中已经有L节点了，自身以F的身份加入集群连接上L之后，L节点会更具自己服务器上最后被提交的提案和F上的提案对比，如果发现F上有上一个Leader周期的事务提案，L会要求F进行回退，回退到一个确实被集群中过半机器提交的最新的事务提案。<br>这就完成了丢弃事务提案的操作。也正是由于ZXID设计的巧妙性，才能完成很多操作。</p><h3 id="选举要求"><a href="#选举要求" class="headerlink" title="选举要求"></a>选举要求</h3><p>关于对选举算法的要求：</p><ol><li>选举出的L节点要持有最高的ZXID</li><li>过半数节点同意</li></ol><p>只要满足这两个要求，你可以自己去实现选举算法。<br>ZK内置的选举算法实现：</p><ol><li>LeaderElection</li><li>FastLeaderElection(默认)</li><li>AuthFastLeaderElection</li></ol><p>我们可以从日志里面也可以发现选举的部分信息<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1646056189216-07abd47c-00dd-47e4-adbc-a7f4b7f03625.png#averageHue=%23171717&clientId=u510575cd-2d84-4&from=paste&height=576&id=u66f7d402&originHeight=576&originWidth=1292&originalType=binary&ratio=1&rotation=0&showTitle=false&size=169214&status=done&style=none&taskId=u69132217-2797-4e73-a42d-0357b167553&title=&width=1292" alt="image.png"></p><h3 id="选举机制概念"><a href="#选举机制概念" class="headerlink" title="选举机制概念"></a>选举机制概念</h3><p>选举中的几种概念</p><ol><li>服务器id myid</li><li>事务id，服务器存放的最大ZXID</li><li>逻辑时钟，发起的投票轮数计数，表示时第几轮的投票。有可能在指定的时间周期内选举不出L节点，需要发送第二轮甚至第三轮的情况，这个时候可能会收到之前的投票情况反馈，比如第二轮投票开始了才收到第一轮的投票反馈，这时候第一轮的票肯定是要作废的。</li><li>选举状态<ol><li>Looking ，竞选状态</li><li>Following，随从状态，同步Leader状态，参与投票</li><li>Observing，观察状态，同步Leader状态，不参与投票。集群太大的时候竞选过程会变得很耗时，所以不需要所有节点都要投票。设置这种状态能有效提高效率</li><li>Leading，领导者状态</li></ol></li></ol><h3 id="选举算法"><a href="#选举算法" class="headerlink" title="选举算法"></a>选举算法</h3><ol><li>每个实例都发起选举自己作为L的投票（自己投票给自己）</li><li>其他服务实例收到投票邀请，比较发起者的事务ID，投较大的那一方，如果相等就比较发起者的服务ID，投较大的一方</li><li>发起者收到大家的投票反馈，看投票数（含自己的票）是否大于集群的节点数的半数，大于就选举成功，担任领导者。未超过半数且领导者未选出，则再次发起投票。</li></ol><p>选举完成的条件就是有节点拥有半数的票。</p><h4 id="流程示例"><a href="#流程示例" class="headerlink" title="流程示例"></a>流程示例</h4><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1646057062009-3b5671bc-52f2-41eb-b42f-b9d909e69746.png#averageHue=%23eeeeee&clientId=u510575cd-2d84-4&from=paste&height=789&id=u8f617345&originHeight=789&originWidth=1674&originalType=binary&ratio=1&rotation=0&showTitle=false&size=355624&status=done&style=none&taskId=u5a0d2076-5286-4c09-9da0-6f78307c7e3&title=&width=1674" alt="image.png"><br>ZK在CAP理论保证了CP，且C只保证了顺序一致性。一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）在选举的时候ZK是不可用的。<br>关于ZK的理论知识部分就大致讲完了，ZK本来不是一个很复杂的组件，工作中如果遇到问题的话，多思考多看看官方文档就OK了。</p>]]></content>
    
    
    <categories>
      
      <category>middileware</category>
      
    </categories>
    
    
    <tags>
      
      <tag>middileware</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kafka</title>
    <link href="/2024/06/16/kafka/"/>
    <url>/2024/06/16/kafka/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"/><p>Kafka是linkedin使用Scala编写具有高水平扩展和高吞吐量的分布式消息系统。目前的Kafka是scale和java混合编程实现的。<br>Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接收者称为Consumer，此外Kafka集群由多个kafka实例组成，每个实例（server）称为broker。<br>无论是Kafka集群，还是producer和consumer都依赖于zookeeper来保证系统可用性，zk为集群保存一些meta信息。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647219047197-c01b0b5f-40f5-4acd-8bdc-9ad8a39d54d4.png#averageHue=%23dddddd&clientId=ue82b16c4-1585-4&from=paste&height=284&id=u6177d17c&originHeight=284&originWidth=853&originalType=binary&ratio=1&rotation=0&showTitle=false&size=54990&status=done&style=none&taskId=ud338bb68-555b-4bce-8757-1f4b665ecfd&title=&width=853" alt="image.png"></p><h1 id="主流MQ的对比"><a href="#主流MQ的对比" class="headerlink" title="主流MQ的对比"></a>主流MQ的对比</h1><p>kafka可以作为大数据系统的组件，正是因为kafka的高吞吐量的特性。<img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647219282859-742854b7-a65a-4bbe-adf5-191e0d4f078d.png#averageHue=%233a8bce&clientId=ue82b16c4-1585-4&from=paste&height=713&id=u3a084205&originHeight=713&originWidth=1770&originalType=binary&ratio=1&rotation=0&showTitle=false&size=471206&status=done&style=none&taskId=u0d4c9f17-d407-4163-9035-98a84c36a00&title=&width=1770" alt="image.png">吞吐量方面：kafka&gt;RabbitMQ&gt;ActiveMQ<br>数据可用性、准确性保证：RabbitMQ&gt;ActiveMQ&gt;kafka<br>kafka注重批量数据，具有高可用（HA）、高水平扩展、高吞吐量的特性。kafka使用的是仿AMQP协议，在AMQP的基础上进行了修改。<br>集群方面，支持但不擅长意味着如果要进行集群就需要耗费相当的精力，可能也不方面扩展。而kafka天生就是以集群的方式存在，线上使用3个kafka集群才是对它的尊重。<br>再拿RocketMQ来对比一下，RocketMQ是阿里在kafka的基础上进行重写实现，所以很多方面都和kafka类似：数据的堆积、大吞吐量。它们两个最大的区别在于场景的适用。RocketMQ会更擅长业务系统，对事务支持很好，而Kafka更擅长数据领域，更关注数据的吞吐。</p><h2 id="Kafka主要特性"><a href="#Kafka主要特性" class="headerlink" title="Kafka主要特性"></a>Kafka主要特性</h2><p>官网称kafka是一个流处理平台，流平台需要如下特性：</p><ul><li>可发布和订阅流数据，类似于消息队列或者消息级消息系统</li><li>以容错的方式存储流数据。关键在于存储二字，kafka可以存储数据。</li><li>流数据产生时就进行处理。kafka提供了一些api（Stream API），可以实现在数据进入在kafka之后不直接消费，而是在topic之间进行数据的处理，比如数据清洗，这样消费者可以消费到被处理过的数据。</li></ul><h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><p>基于kafka构造实时流数据管道，让系统或应用之间可靠地获取数据；<br>构建实时流式应用程序，处理流数据或基于数据做出反应。流数据意味着数据不断产生，不断被处理。在处理数据的时候，可以判断出数据的类型，基于不同的数据做出不同的处理反应。如果消费数据中出现了问题，可以从某个点重新消息纠正，这也是流式应用程序的特点，既可以实时消息，也可以从之间某个数据点重新消费。</p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>kafka实现的是仿AMQP协议，那我们先来看看什么是AMQP协议。</p><h3 id="AMQP协议"><a href="#AMQP协议" class="headerlink" title="AMQP协议"></a>AMQP协议</h3><p>AMQP（Advanced Message Queuing Protocol）是一个提供统一消息服务的标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件而设计。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647220807463-f84c021c-09de-42f5-a43b-1bc76c0048c1.png#averageHue=%23f2f2f2&clientId=ub6769867-b391-4&from=paste&height=195&id=LjHoD&originHeight=195&originWidth=1310&originalType=binary&ratio=1&rotation=0&showTitle=false&size=52942&status=done&style=none&taskId=ub95c3417-8b3e-4f41-bcaf-15412eb28dd&title=&width=1310" alt="image.png"><br>server：AMQP服务器，接收客户端连接，实现AMQP消息队列和路由功能的进程<br>producer：生产者，向Broker发布消息的客户端应用程序<br>consumer：消费者，向消息队列请求消息的客户端应用程序</p><p>kafka仿AMQP，在于kafka的broker是以集群的形式存在，push 和pull操作的都是broker集群。除此以外，kafka是以多播的形式进行的，ActiveMQ和RabbitMQ的消息无论是排队（queue）还是多播消费方式只能被消费一次，而kafka是durable的，消息会存在堆积，那么消息就可以被消费多次，并且kafka还能做到广播消费。</p><h3 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h3><p>数据主题，是kafka中用来代表一个数据流的一个抽象。发布数据时，可以用topic对数据进行分类，也作为订阅数据时的主题。一个Topic同时可以有多个producer、consumer。</p><h3 id="Partation"><a href="#Partation" class="headerlink" title="Partation"></a>Partation</h3><p>每个partition时一个顺序的、不可变的record序列，partition中的record会被分配一个自增长的id，我们称之为offset。</p><h3 id="Record"><a href="#Record" class="headerlink" title="Record"></a>Record</h3><p>每条记录都有key、value、timestamp三个消息<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647228049364-11b89b53-c308-4a7e-87c5-aea09be1bbec.png#averageHue=%23f3f3f3&clientId=ub6769867-b391-4&from=paste&height=412&id=u9cd6aa1c&originHeight=412&originWidth=1347&originalType=binary&ratio=1&rotation=0&showTitle=false&size=105243&status=done&style=none&taskId=u73acc42a-687a-42f1-a255-4796667f6e6&title=&width=1347" alt="image.png"><br>消息在Partition不断递增， 且有序有唯一ID，但是这个有序只保证在Partition层面上的，topic级别以上都不保证有序了。如果你想要保证topic级别下有序，额那就让topic只有一个partition就可以了。其实很多时候我们只需要保证一类数据的有序性，利用hash（key）%Count（partition）将同类的数据放到同一个partiton就可以保证数据的有序性。比如交易数据，我只需要保证股票类有序，基金类有序，而不需要保证全部的有序。</p><p>那么在kafka中如何定位一条数据呢？Topic:Partition:offset。</p><h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h3><p>每个partition还会被非制导其他的服务器作为replication，这是一种冗余备份策略。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647227997992-0d358483-c078-435e-a865-113bf28e649a.png#averageHue=%23d5d5d5&clientId=ub6769867-b391-4&from=paste&height=342&id=p1wNv&originHeight=342&originWidth=1535&originalType=binary&ratio=1&rotation=0&showTitle=false&size=100330&status=done&style=none&taskId=u3cbc2453-0797-4e7d-baf6-f1a922a10c2&title=&width=1535" alt="image.png"></p><p>假如总共有12T的数据需要被存储处理。每个机器只有3T的磁盘容量，那么数据就应该被分成4分，被存储到4个机器上，每份都是3T，我们称其为<strong>数据分片</strong>。每个数据分片都不是完整的数据，4个分片合并才能被称为是完整数据。通过这样的数据分片，我们存储起来了单个机器不能存储的大数据。并且我们提高了并发级别：如果是使用一台能存储12T的数据的机器，那么只能实现单个机器上处理。而分了4个分片之后，原先只能在一个机器上完成的操作，我们能实现4个分片同时处理，且服务器的压力也被分散了，提高了并发级别，读写性能。降低了数据丢失的风险，如果12T数据出现数据损坏，那么整个12T数据将不可用，分为4个分片将这种风险也降低了。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647226891322-81ce1eec-1c72-40e0-850e-dfb30134c4ca.png#averageHue=%23f8f8f7&clientId=ub6769867-b391-4&from=paste&height=377&id=u3988b90e&originHeight=377&originWidth=957&originalType=binary&ratio=1&rotation=0&showTitle=false&size=126331&status=done&style=none&taskId=u6d5d214e-8c4c-44b5-bc9e-860318ac899&title=&width=957" alt="image.png"><br>在此基础上难道没有缺点吗？最大的问题就是数据的可用性非常差。如果分片所属的机器宕机了，那么上面的数据就完全不可用了，并且整个数据的完整性都失去了，这就是可用性差。为此我们将每个机器存储设置为6T，这样每个机器就能多存一份其他分区的备份。对外提供服务的partition（如图中的p0-p3）被称为 Leader Partition ，kafka集群中的leader指的就是这个Leader Partition（LP），这个leader和follower的概念和其他的MQ是有所不同，而Replication Partition（RP）就是Follower Partition（FR）。kafka中的FR只是冗余备份，不提供任何服务。只有在LP宕机之后才由FP来提供，如果由多个FP那么就选举出一个新的分片来做LP去提供服务。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647227542002-b99c3878-7768-4527-8496-dfd0c0ea0eb5.png#averageHue=%23f6f5df&clientId=ub6769867-b391-4&from=paste&height=520&id=u3d1991bd&originHeight=520&originWidth=1113&originalType=binary&ratio=1&rotation=0&showTitle=false&size=179244&status=done&style=none&taskId=u584358e1-d3b9-4e4f-bf24-4951c9fe1aa&title=&width=1113" alt="image.png"></p><h2 id="核心API"><a href="#核心API" class="headerlink" title="核心API"></a>核心API</h2><p>kafka有四个核心api：</p><ul><li>producer：允许一个应用程序发布一串流式的数据到一个或者多个kafka topic。</li><li>consumer：允许一个应用程序订阅一个或多个topic，并且对发布他们的流式数据进行处理</li><li>streams ：允许一个应用程序作为一个流处理器，消息一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，再输入输出流中进行有效的转换。</li><li>connector：允许构建并运行可重用的生产者或者消费者，将kafka topic连接到已存在的应用程序或者数据系统。比如连接到一个关系型数据库，捕捉表的所有变更内容。方便我们连接数据库或其他中间件，比如mysql，redis。这种从别的系统种读取进入kafka的接口在connector称为source，而往外写数据的接口称为sink。这个connector经常被用来数据同步并做数据清洗。</li></ul><p>你可能会对streams API有疑问，我好像自己可以通过producer和consumer来实现类似stream的功能，为什么还要提供一个streams这样一个API呢？<br>实际上streams就是通过producer和consumer来实现的。streams api的作用在于简化topic传递数据之间进行类似java stream一样的操作，封装了这种stream的算子（join、group、filter），这样我们更能集中在数据处理本身。</p><p>下面将详细介绍一下几个API，但是使用的就不会详细说，使用方式基本都是引包然后写点图中差不多的代码，主要是以理解工作原理为主。</p><h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h3><p><img src="https://cdn.nlark.com/yuque/0/2021/png/1980660/1623634608725-a7a483d6-b22e-4af8-bbda-c9153e2bd869.png#averageHue=%23dedede&height=384&id=ddLyI&originHeight=384&originWidth=1396&originalType=binary&ratio=1&rotation=0&showTitle=false&size=174665&status=done&style=none&title=&width=1396" alt="image.png"><br>重点是需要理解下面三个配置： </p><ol><li>batch.size  : Producer会为每个partition维护一个buffer缓冲，用来记录还没有发送的数据，每个缓冲区内存大小用batch.size指定，默认是16k。由于每个partition都有自己的buffer，那么在此时保证了有序性。如果消息的大小超过了buffer的大小，那么消息将不会经过buffer，直接发送出去。</li><li>linger.ms : linger.ms为buffer中的数据在达到batch.size前，需要等待的时间，即buffer的数据最多等待多久发送出去。如果没有配置这个值，那么消息会被直接发送出去，而不需要等到buffer打满。</li><li>asks<br> acks用来配置请求成功的标准</li></ol><ul><li>0：不考虑服务端的响应，直接放到buffer后返回，吞吐量非常大，可用性保证就很差。这种一般用于日志数据。</li><li>1：当写入leader partition成功之后就返回</li><li>all：需要replication写入成功才会返回</li></ul><ol start="4"><li>retries 如果发送消息失败的重试次数</li></ol><p>acks和send方法放到buffer是没有关系的，acks描述的是buffer发送到partition（leader partiton、replication partition）是否成功的返回。设置为1或者all的时候，要等待得到了正确的响应之后，后面batch的数据才会发送出去不然就会阻塞等待直到OOM。<br>每一个partition都有自己对应的buffer。send的方法是异步的，它负责将数据发送到buffer中，producer中的background IO线程来负责发送到partition，这种方式也是提高了吞吐量，于此同时我们也应该注意的是send方法并不一定会成功。<br>partition的个数越多并发量就越高。<br>推送消息有两个条件：buffer满了或者是linger.ms等待时间到了，谁先达到等待条件就被推送。</p><hr><p>指定partition</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">consumer.assign(Arrays.asList(<span class="hljs-keyword">new</span> <span class="hljs-title class_">TopicPartition</span>(<span class="hljs-string">&quot;market_topic&quot;</span>,<span class="hljs-number">0</span>)));<br></code></pre></td></tr></table></figure><p>指定offset</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java">      <span class="hljs-comment">//指定offset。seek一定要在poll方法调用之后才能拿到，否则抛出异常</span><br>Set&lt;TopicPartition&gt; assignement = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashSet</span>&lt;&gt;();<br>      <span class="hljs-keyword">if</span>(assignement.size() == <span class="hljs-number">0</span>)&#123;<br>          consumer.poll(<span class="hljs-number">100</span>);<br>          assignement = consumer.assignment();<br>      &#125;<br>      System.out.printf(<span class="hljs-string">&quot; === consumer assignment : %s \n&quot;</span>,assignement);<br>      consumer.seek(<span class="hljs-keyword">new</span> <span class="hljs-title class_">TopicPartition</span>(TOPIC,<span class="hljs-number">0</span>),<span class="hljs-number">10</span>);<br><br></code></pre></td></tr></table></figure><h3 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h3><p>producer api相对来说会比较简单，consumer是kafka比较复杂的一块。<br>我们知道kafka是一个集群，每个broker上数据都不是完整的，每个机器会存储别的分片数据的备份（replication）来提高可用性。kafka使用zk来存储元数据，比如 broker ID、 HostName、IP:Port，topic的名字，每个topic有多少个Partition，Replication及保存在哪个broker上<br>等元素据都会交由zk来存储维护。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647479064852-6fadcee9-2987-45f2-a2ff-e7a3dc3dff37.png#averageHue=%23e9e9e9&clientId=u3412e353-d070-4&from=paste&height=453&id=u02b3d24a&originHeight=453&originWidth=853&originalType=binary&ratio=1&rotation=0&showTitle=false&size=103790&status=done&style=none&taskId=u8f18e2c1-5620-4a8a-9c80-aeb0744fc87&title=&width=853" alt="image.png"><br>提一句，zk是一个分布式信息协调服务，注册中心、配置中心都只是其中一种用法。zk集群和kafka 、HDFS、ES集群最大的不同就是，zk集群中的每一个节点存储的数据都是完整的，其他的只是存储了一部分数据，是不完整的。zk在设计的时候不追求性能，更保证数据的高可用、数据一致性，每次写入数据都是一个事务，真正做到只有所有zk节点写入同步成功才会返回写入成功，基于这个原因，zk节点越多性能会越差。我们增加zk节点是为了集群的可用性，容灾，而不是为了提高zk的性能。更多zk内容可以看看zookeeper。这里讲的可用性其实是CAP中的P，即分区容错。</p><p>kafka的消费方式和其他的传统MQ（Active）消费方式有所不同。pub数据到ActiveMQ到sub出去拉取被消费，不管是广播还是队列（queue）的消费方式，消息始终只能被消费一次。但是kafka中，因为消息在kafka是会落地的，只要不被清理，数据会一直存在，消费者就可以从特定的点消费，那么消息是可以被消费多次的。<br>在kafka中有个consumer group的概念，里面可以存在多个消费者。每个consumer有且只能消费一个partition，这种使用group的方式可以很好地提高了并发量，且并发度间接受到partition的控制。而partition可以被不同group的consumer消费，但是同一个group下只能被一个consumer消费。这样的机制使得在不同group下的consumer能够消费相同的partition，这样就做到了多次消费。如果group中的consumer数量多余partition，那么多出来的一个consumer会“饿死”，貌似久了会报错，所以一般consumer数量会设置比partition数量少。如果consumer的数量比partition少，那么多的partition会分配给谁这个是随机的，不知道是否可以指定。<br>group中consumer如果挂掉了，那么group会帮我们做容错，分配给正常的节点。且进度并不会从新开始而是继续消费，如果从头消费，那么就可能是我们不希望看到的重复消费了。这个消费进度并不是存在客户端，不会随着客户端宕机而消失。<br>kafka的consumer都是以group的形式来存在的。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647500814342-04b5b7bb-a558-4d8b-9037-09d111a218f5.png#averageHue=%23f7f4f4&clientId=u93a9fbe0-b705-4&from=paste&height=562&id=u0aa39059&originHeight=562&originWidth=1307&originalType=binary&ratio=1&rotation=0&showTitle=false&size=238606&status=done&style=none&taskId=ud1c0de80-f712-412c-91ef-50c921440c0&title=&width=1307" alt="image.png"><br>考虑一个问题，即使不使用group的概念，我们限制一个consumer只能消费一个partition，这样也是可以做到快速消费和负载均衡的。实际上group的作用更多的不同的消费需求做消费（进度）隔离，实现重复消费。在kafka中partition里面数据有offset的概念，每个offset（偏移量）下就是代表的一个记录、一条数据。我们直到通过topic+partitionId+offset可以定位一条数据，每个consumer在消费的时候会有自己的消费进度。可能比如 p1:8 ；p2:1;p3:1;p4:0  表示partition1 消费到第8条数据,p2消费到1以此类推。不同的group可能会存在消费进度不一样的情况。groupId相同的consumer属于同一个group，同一个group下共享一个消费进度，所以消费进度是包含了所有partition的消费情况的（即P1-P4，而不是独立开来，这点要确认）。在kafka早期的版本这个消费进度是保存在zk中，因为这个消费进度会频繁更新，zk的压力会增大，后来kafka使用了一个专门的topic（_offset_topic_）来存储。<br>更准确一点，保存的消费进度需要有什么东西呢？groupId,topic_name,partition_id,offset。宕机之后接收的consumer就能通过这些信息继续消费。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647499831481-db7e1909-f57d-4aaa-a700-60ab080ec524.png#averageHue=%23f7f7f7&clientId=u93a9fbe0-b705-4&from=paste&height=543&id=u3eefc778&originHeight=543&originWidth=1323&originalType=binary&ratio=1&rotation=0&showTitle=false&size=195767&status=done&style=none&taskId=u5514e46b-35fc-4935-b06f-07b2a7bbafd&title=&width=1323" alt="image.png"></p><p>实例代码：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647506436655-6783d8cc-7d83-424c-bef5-231aad212651.png#averageHue=%23fdfcfb&clientId=u0a311782-ace7-4&from=paste&height=754&id=u4ec02400&originHeight=754&originWidth=1813&originalType=binary&ratio=1&rotation=0&showTitle=false&size=637597&status=done&style=none&taskId=uf38fae56-410b-4596-a114-83455cdabc1&title=&width=1813" alt="image.png"><br>prop中有个key.deserializer和value.deserializer，数据在kafka以二进制的方式存在磁盘，我们传送过来的数据可能是各种类型，String、Double都可能，这时候我们需要进行序列化，在producer端进行序列化，消费者端进行反序列化。这是个加密解密的过程。这个过程包括两个过程，一个key，一个value，为什么会有两个东西呢？难道kafka的存储方式是K-V的形式存储的？如果是K-V存储方式，那么这个K-V的key和代码中的key是同一个东西吗？K-V的value是消息本身，那么K自然是要能唯一标识这个消息的K，我们直到在kafka中唯一标识消息的东西是topic\partition\offset这三样组成，所以这个K就应当是这三个东西，但是实际上实现可能并不是这样，我们所说的topic\partition\offset是一种方便我们理解的一种逻辑上的表达。但实际上kafka并不是K-V的存储，并不能通过K去查询。而props中的key和kafka的K-V没有任何关系。<br>像实例代码中key就是写死的常量。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647507391615-5bd4152d-98e3-4232-b4ce-fd23e984990a.png#averageHue=%23fdfbf7&clientId=u0a311782-ace7-4&from=paste&height=150&id=u6c68718f&originHeight=150&originWidth=1084&originalType=binary&ratio=1&rotation=0&showTitle=false&size=66709&status=done&style=none&taskId=uf317c64f-a91a-42e1-a0d8-55bea4c261b&title=&width=1084" alt="image.png"><br>我们在往kafka写入数据的时候，可能存在多个partition，那么就设计消息分发，我们可能希望某个消息固定发送到一个partition上，那么这个key就是用在这个时候，如果key&#x3D;null，代表的是轮询partition存储，当key!&#x3D;null的时候就是做hash计算%count(partitions)。这个是key的作用。这就是为什么每次使用kafka去写入数据都可以决定写不写这个key，kafka对这个key的要求不严格，甚至key是同一个。估计是因为同时配置了key和value，让人不禁想起了map这种基础的结构带来了误解。我们也可以指定partition。key可以帮助我们控制数据写入的方式，间接上控制了数据的顺序。<br>poll方法可以传入一个timeoutMs，在没有到达超时时间，客户端会保持长连接直到超时，减少连接的次数。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647508576860-7e87403d-17bf-46a3-acfe-96da4eff9d7d.png#averageHue=%23f2f1e1&clientId=u0a311782-ace7-4&from=paste&height=594&id=u9d52f02e&originHeight=594&originWidth=1226&originalType=binary&ratio=1&rotation=0&showTitle=false&size=408201&status=done&style=none&taskId=u1b9b604f-8c83-4176-bf8c-7dc6485a610&title=&width=1226" alt="image.png"><br>我们接收到的是ConsumerRecords对象，api就不过多介绍了。<br>关注这行代码：<code>props.setProperty(&quot;enable.auto.commit&quot;,&quot;true&quot;)</code> 这个并不是一个ack，而是commit消费进度，这里代表是自动提交消费进度。<strong>自动提交会在每次poll的时候会将上次的消费进度提交过去</strong>。正常关闭consumer的话在关闭的时候也会提交消费进度。<br>这个autocommit有个两个坑：</p><ol><li>会导致重复消费问题。在最后一次处理完数据，刚刚好写入DB完成之后，consumer宕机了，那么22就没有被提交到服务端，服务端只有16，那么就会重复拉取17-22的数据，那么DB就可能出现两次数据。</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647567397657-3cd1cce7-ef9a-45a8-9f19-97a03511c579.png#averageHue=%23f4efee&clientId=u93b90d58-7339-4&from=paste&height=866&id=g1Mus&originHeight=866&originWidth=1430&originalType=binary&ratio=1&rotation=0&showTitle=false&size=439528&status=done&style=none&taskId=uc951d299-d084-487b-8121-532c66f2614&title=&width=1430" alt="image.png"></p><ol start="2"><li>同样是自动提交，但是这次是我们设置了个<code>minBatchSize</code>，用意是当数据满足20+条的时候才插入到DB中。如果此时你还是使用自动配置的设置，那么两边的落地的方式时不一致的。kafka依然是提交上一次的，而DB却是分批次提交，这样就可能出现一边提交一边未提交的情况，在这段时间如果系统出现故障，那就出现了不一致的问题。</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647568917802-845217f4-8a49-4eec-8fb7-66957282a4e3.png#averageHue=%23fcfcfb&clientId=u93b90d58-7339-4&from=paste&height=859&id=ub5e483b1&originHeight=859&originWidth=1550&originalType=binary&ratio=1&rotation=0&showTitle=false&size=790909&status=done&style=none&taskId=u69d883e7-8997-4e35-ab43-7d7a209a0ac&title=&width=1550" alt="image.png"><br>这个问题只要不自动提交就能处理。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647587716305-40cf46bc-8f41-4021-8e18-2e3fc09f01db.png#averageHue=%23fcfcfa&clientId=u231c5563-e539-4&from=paste&height=847&id=u98a70409&originHeight=847&originWidth=1704&originalType=binary&ratio=1&rotation=0&showTitle=false&size=704727&status=done&style=none&taskId=u7bba69fa-0fb0-4b3f-8275-96092882a59&title=&width=1704" alt="image.png"><br>从上面两个例子我们可以看出来，自动提交存在的问题还是比较大，数据如果比较重要的话，还是自己手动控制提交的好。AutoCommit给予我们的只是操作上的方便而已。虽然还是分了两步，但是比自动提交稍微可控一点。我们不必害怕插入失败commit成功的问题，因为只要加个判断，在insertiToDB成功才commit进度的方式就能避免这个问题。不要考虑使用事务控制，因为kafka所解决的问题吞吐量大，如果使用事务那就有点反其道而行之的感觉了。但是有可能出现insertDB成功，但是commit失败的情况，这种情况就会出现数据重复的问题，的确需要处理，DB做幂等性控制，比如设置唯一主键的方式，让kafka过来的重复数据无法commit，但是这种做法又会影响到insertDB成功之后commit的流程，不太好解决。<br>手动提交保证的语义是at least once，即最少一次。而自动提交会出现丢数据也会出现数据重复的问题。</p><p><strong>我们要怎么样保证消费数据恰好落地一次，消费一次呢？</strong><br>那我们就需要就需要做到数据落地以及消费进度提交两个操作的原子性，这样就可以保证exactly once。<br>有个方案，我们把数据落地到mysql，那么就需要设计这样的表，第一列存储kafka Info，这个kafka info就是当前的offset，我们每一次把数据都往mysql写的时候，要知道数据都有对应的partition和offset，这个kafka info就是当前数据的pid和offset。当topic确认之后，通过pid和offset唯一确认一个数据。<br>这种做法等于说是用来解决我们上面提出的幂等性控制问题， 如果我们直接对消息落地的表操作，那就会出现影响程序流程的问题，那么以这种引入第二张表的形式来控制。这种做法，我们可以不用是消息进度落地，却能够让数据落地，并且记录了消费进度与数据的关系。当consumer宕机后重新启动消费的时候，我们就可以去差这个表，查到对应pid中的消费到哪个offset，就可以接着消费，这样就不会重复消费，也不会丢失数据。消费进度和数据落地在这张表和为了一个操作，并且没有依赖事务，所以不依赖能提供事务的mysql等工具，在某些不支持事务的工具中（hive）也能实现。注意的是consumer重新启动的时候就需要先去这个表里面查询。现在看来是比较理想的解决方案。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647653848911-08fb8a05-8165-4a50-bc49-b8d5ee0ccfab.png#averageHue=%23f7f7f7&clientId=uad2d1815-38f4-4&from=paste&height=784&id=ucd395811&originHeight=784&originWidth=1425&originalType=binary&ratio=1&rotation=0&showTitle=false&size=131743&status=done&style=none&taskId=ub7fbfced-b5b3-451a-b770-66fa4d75129&title=&width=1425" alt="image.png"></p><p>我们使用自动或手动地提交进度，甚至在外部保存消费进度，主要目的就是为了重新消费的时候，知道从哪里开始消费。</p><p>kafka中怎么样做到数据不丢失？<br>ACK 设置成all， 保证消息存储到replication partition。设置合理ISR，保证多个replication partition都同步LP成功。</p><h3 id="Stream-API"><a href="#Stream-API" class="headerlink" title="Stream API"></a>Stream API</h3><p>stream api是用来做流处理的,不间断地处理数据。<br>那么流程序的作用或者意义是什么呢？（java lambda 是一种语法，和流处理没有太大的关系）<br>流程序是和批处理程序相对的。数据一般以流的形式产生的，在传统处理方式中，不断产生的数据会被累计起来，成为一个大的数据集（Master DataSet），那么我们就对这些数据进行批处理（Batch），得到新的结果集（PreComputed views），然后我们就可以展示或者查询、使用这个结果集。这种批处理的处理方式存在一个问题，那就是数据存在延迟，数据的产生和结果查询存在时间差，不是实时的。且数据量一般会很大，都是在夜间进行数据抽取，防止影响业务正常运行。流处理就是为了补充这个实时性处理问题，在数据产生的时候就开始不停地process，并且实时地写入到增量的结果集（increment），这个结果集也是要支持实时写入，这个结果集在实时变化，进行查询得到的也是实时的数据。这就是流程序的作用。<br>这种架构就是Lambda架构。和java lambda是没有关系的。在以前流程序是不稳定的，很有可能一个错误的数据就导致系统崩溃，所以批处理和流程序一起合作处理，批处理保证数据一定会存在，而流处理保证数据的实时性。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647655810386-1c86ac8d-dc80-4a81-a306-95020f4d7881.png#averageHue=%23f9f8f8&clientId=uad2d1815-38f4-4&from=paste&height=756&id=u8add80e3&originHeight=756&originWidth=1862&originalType=binary&ratio=1&rotation=0&showTitle=false&size=234184&status=done&style=none&taskId=u0c9f427a-4f84-4721-9202-b78a0f0cc7a&title=&width=1862" alt="image.png"><br>比较麻烦的是，我们要同时维护两套系统，无论系统业务逻辑变更、新增都需要对两个系统做维护，比较麻烦。<br>kafka的流程序可能是比较简单，并没有什么值得讨论的东西。<br>当前流行的架构是Flink或者Spark。流程序的数据会被放入到kafka里面，起到流数据存储的作用，数据会在kafka落地并保证顺序，然后使用比较流行的大数据技术，比如早期的spark 或 出现的较晚但是阿里在推的Flink，它们基于一套api就能实现流处理和批处理，所谓流批一体。Spark的流批一体做的没有Flink完善，Flink的有点就是这，这种架构省去了两次开发的麻烦。并且没有kafka，spark&#x2F;Flink如果顶不住或者其他的情况宕机了，那么数据就真丢了，有了kafka之后就算宕机了，重新启动再消费就行了。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647656288234-e0b38ceb-ab5e-4cf8-81c2-de13199a3974.png#averageHue=%23fbfbfb&clientId=uad2d1815-38f4-4&from=paste&height=743&id=ud348af7f&originHeight=743&originWidth=1800&originalType=binary&ratio=1&rotation=0&showTitle=false&size=163564&status=done&style=none&taskId=ub62bf5f2-4390-49c3-8230-105a9b5cbcb&title=&width=1800" alt="image.png"></p><p>正统的流处理程序有 Spark、Flink、Storm。这些平台的使用都是有成本的，都是需要从新搭建的。kafka的stream api就是个java程序，使用起来方便。这可能就是kafka为什么自称能做流处理的原因吧。<br>api的使用就不多讲了，直接看看代码：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1980660/1647658566634-8adccad3-751b-42f6-9a72-03c8d447d56b.png#averageHue=%23f2f0e5&clientId=uad2d1815-38f4-4&from=paste&height=779&id=u692cfc5e&originHeight=779&originWidth=1432&originalType=binary&ratio=1&rotation=0&showTitle=false&size=758215&status=done&style=none&taskId=u5de71d45-bc24-46a1-90d8-b4b668c8388&title=&width=1432" alt="image.png"><br>介绍一下，首先要写一些配置参数，比如程序名称，连接参数，key value的序列化。创建一个KStream。stream api需要从一个input topic输入，然后输出到一个output topic，这两个都需要配置，<code>KStream&lt;String,String&gt; textLines = builder.stream(stram-plaintxt-input)</code> 就是指定了input topic，这样我们就拿到了一个流，接下来对流进行处理。示例代码我们是想进行count，count完成放到一个topic中。<br>可以自己试着实现一个，跑一下理解会更深刻。</p><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>kafka的应用场景比较多，这里详细介绍一下，大多数情况下是作为带存储的消息队列在使用，官方说可以作为流数据处理平台。</p><h2 id="消息系统"><a href="#消息系统" class="headerlink" title="消息系统"></a>消息系统</h2><p>消息系统被用于各种场景，如解耦数据生产者，缓存未处理的消息。kafka可以作为传统消息系统的替代者，与传统消息系统相比，kafka有更好的吞吐量、更好的可用性， 这有利于处理大规模的消息。还有更大的区别在于消费者方面，具体可以看consumer。<br>根据经验，通常消息传递对消息吞吐量要求比较低，但可能要求较低的端到端延迟，并经常依赖kafka可靠的durable机制。<br>在这方面，kafka可以和传统的消息传递系统ActiveMQ和RabbitMQ媲美。高并发的场景下，其他的MQ在放入到MQ（partition级别）时还能做到保证有序，但是高并发多个消费者消费的情况下（异步），是没有办法保证消息消费的顺序的。而kafka在消费端可以通过一个消费者消费一个partition的方式保证消费有序。值得注意的是这种保证顺序的方案。</p><h2 id="存储系统"><a href="#存储系统" class="headerlink" title="存储系统"></a>存储系统</h2><p>写入到kafka上的数据是会落地到磁盘上的，并且有冗余备份，kafka允许producer等待确认，通过配置，可实现直到所有的replication完成复制才算写入完成，这样可以保证数据的可用性。<br>Kafka认真对待存储，并允许client自行控制读取位置，你可以认为kafka是一种特殊的文件系统，它能够提供高性能、低延迟、高可用的日志提交存储。<br>kafka使用了顺序读与顺序写，所以很快。实际上kafka的存储是建立在<strong>磁盘+内存</strong>两者上，同时实现的高速读取与写入，最终落地磁盘，这样就能实现数据重复消费。<br>kafka是一种<strong>流存储</strong>系统，和其他传统的数据存取系统相比（hdfs、mysql），流存储的特别是保证了数据落地的顺序，这样流程序消费数据的时候就能进行数据回溯，即如果数据在某个点出现问题，那么程序能够找到错误的点，从这个数据点开始继续往前消费，甚至可以从整个数据从头开始消费（回放），这就是顺序存储的好处。</p><h2 id="日志聚合"><a href="#日志聚合" class="headerlink" title="日志聚合"></a>日志聚合</h2><p>这个可能是比较勉强的用法，只能说可以做，但是不合适。<br>日志系统一般需要如下功能：日志收集、清洗、聚合、存储、展示。而kafka第一件收集就没办法做到。一般来说主机上的log文件存在于一个file中，kafka没法把主机的文件送过来，这个一般由Filebeat\Flume来做，它们就像一个传感器一样，一旦文件由新的内容就会把数据发送出来。<br>日志发送出来之后需要做数据清洗，kafka确实可以帮助我们清洗日志。日志传送过来的之后存在一个topic之后，调用stream来清洗在方法另一个topic，但是这种清洗日志的方式不是很好，每一个日志都要启动一个stream API程序，开启一个java进程来处理，这对于日志数据处理来说代价是有点昂贵。更常规的方案是用logstash来正则匹配。<br>日志聚合就是把不同类型的日志来分流或聚合。<br>现在做日志聚合比较流行的方案是ELK。在每个机器上装个Filebeat agent，由Filebeat来读取数据并发送到Logstash清洗数据，清洗完成之后放到ES存储，由K8S来帮助分析、展示、告警。这个方案有个问题，生产上云主机比较多，产生日志非常多，logstash可能扛不住，它可能出现性能瓶颈。这时候使用kafka部署在filebeat和logstash之间，让logstash慢慢解析，不至于一下子把logstash压垮，kafka类似buffer的作用。</p><h2 id="跟踪网站活动"><a href="#跟踪网站活动" class="headerlink" title="跟踪网站活动"></a>跟踪网站活动</h2><p>kafka最初始作用就是将用户行为跟踪管道重构为一组实时发布-订阅源。把网站活动（浏览网页、搜索或者其他的用户操作）发布到中心topic，每种活动类型对应一个topic。基于这些订阅源，能够实现一系列用例，如实时处理、实时监控、批量地将kafka的数据加载到Hadoop或离线数据仓库系统，进行离线数据处理并生成报告。<br>每个用户浏览网页时产生的活动信息是非常大，kafka很适应这种数据场景。</p><h2 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h2><p>使用stream API可以完成。但是使用场景在于数据量下， 并且不想搭建一个完整的流处理平台，需求简单。<br>Kafka社区认为仅仅提供数据生成、消费机制是不够的，他们还需要提供流数据实时处理机制，从0.10.0.0开始，kafka通过提供Streams API来提供轻量，但功能强大的流处理。实际上是Streams API帮助解决流引用中一些棘手的问题，比如：处理无序的数据，代码变化后再次处理数据，进行有状态的流式计算。<br>Streams API的流处理包含多个阶段，从input topics消费数据，做各种处理，将结果写入到目标topic，stream是API基于kafka提供的核心原语构建，它使用kafka consumer、producer来输入、输出，用kafka来做状态存储。<br>流处理框架：flink 、spark streaming、Storm、Samza才是正统的流处理框架，kafka在流处理中更多的是扮演流存储的角色。</p><p>kafka什么时候删除数据？<br>有个配置叫… ，可以配置保留几天的数据。</p><h1 id="Kafka-connect数据传输工具"><a href="#Kafka-connect数据传输工具" class="headerlink" title="Kafka connect数据传输工具"></a>Kafka connect数据传输工具</h1><p>这节讲了大半节zk，然后讲了consumer group。这个数据传输工具没有明讲。</p><h1 id="Kafka-Stream架构"><a href="#Kafka-Stream架构" class="headerlink" title="Kafka Stream架构"></a>Kafka Stream架构</h1><p>传统的MQ在publish消息到MQ后，我们有两种方式sub，一种是拉取（pull）一种是推送（push），不管哪种方式每一个消费端都是消费了MQ中的部分数据。 kafka使用group来完成数据的重复利用，rocketMQ也有同样的group机制。</p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><p>怎么消费效率更高？<br>那就让consumer和partition数量对等，没有什么优化的空间。除了并发度外，还有吞吐的问题，可以调大消费批次，这样拉取的数据更多，那么在消费者性能足够的情况下也可以提高消费效率。</p>]]></content>
    
    
    <categories>
      
      <category>middileware</category>
      
    </categories>
    
    
    <tags>
      
      <tag>middileware</tag>
      
      <tag>kafka</tag>
      
      <tag>mq</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/06/15/hello-world/"/>
    <url>/2024/06/15/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
